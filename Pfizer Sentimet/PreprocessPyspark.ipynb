{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "from pyspark.ml.feature import RegexTokenizer\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.ml.linalg import Vectors\n",
    "import re\n",
    "from pymongo import MongoClient\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "from pyspark.ml.feature import RegexTokenizer\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.ml.linalg import Vectors\n",
    "import re\n",
    "from pymongo import MongoClient\n",
    "\n",
    "class Preprocessor:\n",
    "    def __init__(self):\n",
    "        self.stemmer = PorterStemmer()\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    def remove_hashtags(self, text):\n",
    "        return re.sub(r'#\\w+', '', text)\n",
    "\n",
    "    def remove_emojis(self, text):\n",
    "        emoji_pattern = re.compile(\"[\"\n",
    "                                  u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                                  u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                                  u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                                  u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                                  \"]+\", flags=re.UNICODE)\n",
    "        return emoji_pattern.sub(r'', text)\n",
    "\n",
    "    def preprocess_text(self, text):\n",
    "        text = self.remove_hashtags(text)\n",
    "        text = self.remove_emojis(text)\n",
    "        # Add more preprocessing steps as per your requirement\n",
    "        return text\n",
    "\n",
    "    def stem(self, text):\n",
    "        words = text.split()\n",
    "        stemmed_words = [self.stemmer.stem(word) for word in words]\n",
    "        return ' '.join(stemmed_words)\n",
    "\n",
    "    def lemmatize(self, text):\n",
    "        words = text.split()\n",
    "        lemmatized_words = [self.lemmatizer.lemmatize(word) for word in words]\n",
    "        return ' '.join(lemmatized_words)\n",
    "\n",
    "    def preprocess_dataframe(self, dataframe):\n",
    "        preprocess_text_udf = udf(self.preprocess_text)\n",
    "        stem_udf = udf(self.stem)\n",
    "        lemmatize_udf = udf(self.lemmatize)\n",
    "\n",
    "        # Remove hashtags and emojis\n",
    "        dataframe = dataframe.withColumn('preprocessed_text', preprocess_text_udf(col('text')))\n",
    "\n",
    "        # Stemming\n",
    "        dataframe = dataframe.withColumn('stemmed_text', stem_udf(col('preprocessed_text')))\n",
    "\n",
    "        # Lemmatization\n",
    "        dataframe = dataframe.withColumn('lemmatized_text', lemmatize_udf(col('preprocessed_text')))\n",
    "\n",
    "        return dataframe\n",
    "\n",
    "class LabelEncoder:\n",
    "    def __init__(self):\n",
    "        self.label_indexer = StringIndexer(inputCol='sentiment', outputCol='label')\n",
    "\n",
    "    def encode_labels(self, dataframe):\n",
    "        indexed_data = self.label_indexer.fit(dataframe).transform(dataframe)\n",
    "        return indexed_data\n",
    "\n",
    "class SentimentAnalyzer:\n",
    "    def __init__(self):\n",
    "        self.spark = SparkSession.builder.appName('SentimentAnalysis').getOrCreate()\n",
    "\n",
    "    def load_data_from_csv(self, file_path):\n",
    "        dataframe = self.spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "        return dataframe\n",
    "\n",
    "    def create_pipeline(self):\n",
    "        tokenizer = RegexTokenizer(inputCol='lemmatized_text', outputCol='tokens', pattern='\\\\W')\n",
    "        stopword_remover = StopWordsRemover(inputCol='tokens', outputCol='filtered_tokens')\n",
    "        count_vectorizer = CountVectorizer(inputCol='filtered_tokens', outputCol='features')\n",
    "        logistic_regression = LogisticRegression(featuresCol='features', labelCol='label')\n",
    "\n",
    "        pipeline = Pipeline(stages=[tokenizer, stopword_remover, count_vectorizer, logistic_regression])\n",
    "        return pipeline\n",
    "\n",
    "    def train_model(self, dataframe):\n",
    "        pipeline = self.create_pipeline()\n",
    "        model = pipeline.fit(dataframe)\n",
    "        return model\n",
    "\n",
    "    def evaluate_model(self, model, dataframe):\n",
    "        evaluator = BinaryClassificationEvaluator(labelCol='label', rawPredictionCol='rawPrediction', metricName='areaUnderROC')\n",
    "        predictions = model.transform(dataframe)\n",
    "        roc_auc = evaluator.evaluate(predictions)\n",
    "        return roc_auc\n",
    "\n",
    "    def save_datewise_average_sentiment(self, dataframe, collection_name):\n",
    "        avg_sentiments = dataframe.groupBy('date').agg({'sentiment_score': 'avg'})\n",
    "        sentiment_data = []\n",
    "        for row in avg_sentiments.collect():\n",
    "            date = row['date']\n",
    "            avg_sentiment = row['avg(sentiment_score)']\n",
    "            sentiment_data.append({'date': date, 'average_sentiment': avg_sentiment})\n",
    "\n",
    "        client = MongoClient()\n",
    "        db = client['sentiment_analysis']\n",
    "        collection = db[collection_name]\n",
    "        collection.insert_many(sentiment_data)\n",
    "\n",
    "    def run_sentiment_analysis(self, file_path, collection_name):\n",
    "        dataframe = self.load_data_from_csv(file_path)\n",
    "        \n",
    "        preprocessor = Preprocessor()\n",
    "        dataframe = preprocessor.preprocess_dataframe(dataframe)\n",
    "        \n",
    "        label_encoder = LabelEncoder()\n",
    "        dataframe = label_encoder.encode_labels(dataframe)\n",
    "        \n",
    "        model = self.train_model(dataframe)\n",
    "        roc_auc = self.evaluate_model(model, dataframe)\n",
    "        \n",
    "        self.save_datewise_average_sentiment(dataframe, collection_name)\n",
    "        \n",
    "        print('ROC AUC:', roc_auc)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "# Read data from CSV file\n",
    "df = spark.read.csv(\"PFV.csv\", header=True, inferSchema=True)\n",
    "\n",
    "\n",
    "# Tokenize the text column\n",
    "tokenizer = RegexTokenizer(inputCol=\"text\", outputCol=\"tokens\", pattern=\"\\\\W\")\n",
    "df = tokenizer.transform(df)\n",
    "\n",
    "# Remove stopwords\n",
    "stopwords_remover = StopWordsRemover(inputCol=\"tokens\", outputCol=\"filtered_tokens\")\n",
    "df = stopwords_remover.transform(df)\n",
    "\n",
    "\n",
    "\n",
    "# Label encoding\n",
    "label_encoder = StringIndexer(inputCol=\"sentiments\", outputCol=\"encoded_label\")\n",
    "df = label_encoder.fit(df).transform(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+--------------------+----------+--------------------+--------------------+-----+\n",
      "|_c0|      date|                text|sentiments|               words|      filtered_words|label|\n",
      "+---+----------+--------------------+----------+--------------------+--------------------+-----+\n",
      "|  0|2022-12-19|#Hungary had rece...|       Pos|[hungary, had, re...|[hungary, receive...|  0.0|\n",
      "|  1|2023-03-17|This might help @...|       Pos|[this, might, hel...|[might, help, mic...|  0.0|\n",
      "|  2|2022-07-03|#HongKong and #Ma...|       Neg|[hongkong, and, m...|[hongkong, macau,...|  1.0|\n",
      "|  3|2023-01-13|I got the first d...|       Neg|[i, got, the, fir...|[got, first, dose...|  1.0|\n",
      "|  4|2021-06-30|.@UPS delivers fi...|       Pos|[ups, delivers, f...|[ups, delivers, f...|  0.0|\n",
      "|  5|2021-06-01|@BorisJohnson GIV...|       Pos|[borisjohnson, gi...|[borisjohnson, gi...|  0.0|\n",
      "|  9|2021-06-24|Got my 1st #Pfize...|       Pos|[got, my, 1st, pf...|[got, 1st, pfizer...|  0.0|\n",
      "| 10|2022-02-27|#Moderna and #Pfi...|       Pos|[moderna, and, pf...|[moderna, pfizerb...|  0.0|\n",
      "| 14|2021-11-21|#NSTworld The fir...|       Neg|[nstworld, the, f...|[nstworld, first,...|  1.0|\n",
      "| 15|2022-02-13|Protocol for #COV...|       Pos|[protocol, for, c...|[protocol, covidv...|  0.0|\n",
      "| 16|2021-11-30|@JanetEBaldwin @U...|       Neg|[janetebaldwin, u...|[janetebaldwin, u...|  1.0|\n",
      "| 17|2021-08-01|Just got my first...|       Pos|[just, got, my, f...|[got, first, covi...|  0.0|\n",
      "| 18|2022-01-30|What an experienc...|       Neg|[what, an, experi...|[experience, reme...|  1.0|\n",
      "| 19|2021-10-02|Our reader is wor...|       Pos|[our, reader, is,...|[reader, working,...|  0.0|\n",
      "| 21|2022-03-17|Delays in the vac...|       Neg|[delays, in, the,...|[delays, vaccine,...|  1.0|\n",
      "| 22|2022-07-06|@moderna is stepp...|       Neg|[moderna, is, ste...|[moderna, steppin...|  1.0|\n",
      "| 23|2022-08-06|QUESTION ðââ...|       Pos|[question, which,...|[question, vaccin...|  0.0|\n",
      "| 24|2022-01-17|It was my first d...|       Pos|[it, was, my, fir...|[first, dose, pfi...|  0.0|\n",
      "| 25|2022-08-19|Cherokee Nation l...|       Neg|[cherokee, nation...|[cherokee, nation...|  1.0|\n",
      "| 26|2021-09-22|So after Brexit w...|       Neg|[so, after, brexi...|[brexit, war, dos...|  1.0|\n",
      "+---+----------+--------------------+----------+--------------------+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer, RegexTokenizer, StopWordsRemover\n",
    "\n",
    "# Read data from CSV\n",
    "df = spark.read.csv(\"PFV.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Prepare the pipeline stages\n",
    "tokenizer = RegexTokenizer(inputCol=\"text\", outputCol=\"words\", pattern=\"\\\\W\")\n",
    "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered_words\")\n",
    "indexer = StringIndexer(inputCol=\"sentiments\", outputCol=\"label\", handleInvalid=\"skip\")\n",
    "\n",
    "# Apply the transformations\n",
    "pipeline = Pipeline(stages=[tokenizer, remover,  indexer])\n",
    "model = pipeline.fit(df)\n",
    "transformed_data = model.transform(df)\n",
    "\n",
    "# Show the transformed data\n",
    "transformed_data.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+\n",
      "|      date|         avg(label)|\n",
      "+----------+-------------------+\n",
      "|2023-01-21| 0.3333333333333333|\n",
      "|2021-11-03|                0.5|\n",
      "|2022-10-05|                0.4|\n",
      "|2023-05-01| 0.6363636363636364|\n",
      "|2023-05-18|0.45454545454545453|\n",
      "|2023-04-21| 0.5833333333333334|\n",
      "|2023-04-17| 0.4166666666666667|\n",
      "|2022-10-07| 0.7857142857142857|\n",
      "|2021-12-23|                0.5|\n",
      "|2023-04-28| 0.5555555555555556|\n",
      "|2022-05-17| 0.5384615384615384|\n",
      "|2023-02-10|0.45454545454545453|\n",
      "|2022-03-30|                0.4|\n",
      "|2021-10-25|0.45454545454545453|\n",
      "|2021-11-15| 0.5714285714285714|\n",
      "|2021-08-30| 0.7777777777777778|\n",
      "|2023-03-11|                0.5|\n",
      "|2023-04-26| 2.3636363636363638|\n",
      "|2023-05-04|                0.5|\n",
      "|2023-03-10| 0.7142857142857143|\n",
      "+----------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "avg_sentiments = transformed_data.groupBy('date').agg({'label': 'avg'})\n",
    "avg_sentiments.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pandas_df = avg_sentiments.toPandas()\n",
    "pandas_df.to_csv('PFV_op.csv', index=False, encoding='utf-8')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SAVE TO MOGODB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pymongo.results.InsertManyResult at 0x10966bc8>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pymongo import MongoClient\n",
    "\n",
    "# Connect to MongoDB\n",
    "client = MongoClient('mongodb://localhost:27017')\n",
    "\n",
    "# Access the desired database and collection\n",
    "db = client['PFV']\n",
    "collection = db['Sentiment_Date_wise']\n",
    "\n",
    "# Convert pandas DataFrame to a list of dictionaries\n",
    "data_dict = pandas_df.to_dict(orient='records')\n",
    "\n",
    "# Insert the data into the MongoDB collection\n",
    "collection.insert_many(data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
